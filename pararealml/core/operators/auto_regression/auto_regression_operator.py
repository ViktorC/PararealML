from typing import Union, Tuple, Callable, Optional, Protocol

import numpy as np

from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor

from pararealml.core.constrained_problem import ConstrainedProblem
from pararealml.core.initial_condition import DiscreteInitialCondition
from pararealml.core.initial_value_problem import InitialValueProblem
from pararealml.core.operator import Operator, discretise_time_domain
from pararealml.core.solution import Solution


class SKLearnRegressor(Protocol):
    def fit(self, x, y, sample_weight=None): ...
    def predict(self, x): ...
    def score(self, x, y, sample_weight=None): ...


RegressionModel = Union[SKLearnRegressor, KerasRegressor]


class AutoRegressionOperator(Operator):
    """
    A supervised machine learning operator that uses auto regression to model
    a high fidelity operator for solving initial value problems.
    """

    def __init__(
            self,
            d_t: float,
            vertex_oriented: bool):
        """
        :param d_t: the temporal step size to use
        :param vertex_oriented: whether the operator is to evaluate the
            solutions of IVPs at the vertices or cell centers of the spatial
            meshes
        """
        if d_t <= 0.:
            raise ValueError

        self._d_t = d_t
        self._vertex_oriented = vertex_oriented
        self._model: Optional[RegressionModel] = None

    @property
    def d_t(self) -> float:
        return self._d_t

    @property
    def vertex_oriented(self) -> Optional[bool]:
        return self._vertex_oriented

    @property
    def model(self) -> Optional[RegressionModel]:
        """
        The regression model behind the operator.
        """
        return self._model

    @model.setter
    def model(self, model: Optional[RegressionModel]):
        self._model = model

    def solve(
            self,
            ivp: InitialValueProblem,
            parallel_enabled: bool = True
    ) -> Solution:
        if self._model is None:
            raise ValueError

        cp = ivp.constrained_problem
        diff_eq = cp.differential_equation

        time_points = discretise_time_domain(ivp.t_interval, self._d_t)

        y_shape = cp.y_shape(self._vertex_oriented)

        x = self._create_input_placeholder(cp)
        x = np.concatenate(
            (x, np.empty((x.shape[0], diff_eq.y_dimension))),
            axis=-1)
        y = np.empty((len(time_points) - 1,) + y_shape)

        y_i = ivp \
            .initial_condition \
            .discrete_y_0(self._vertex_oriented) \
            .reshape(-1, diff_eq.y_dimension)

        for i, t_i in enumerate(time_points[:-1]):
            x[:, 0] = t_i
            x[:, 1 + diff_eq.x_dimension:] = y_i
            y_i = self._model.predict(x).reshape(
                x.shape[0], diff_eq.y_dimension)
            y[i, ...] = y_i.reshape(y_shape)

        return Solution(
            cp,
            time_points[1:],
            y,
            vertex_oriented=self._vertex_oriented,
            d_t=self._d_t)

    def train(
            self,
            ivp: InitialValueProblem,
            oracle: Operator,
            model: RegressionModel,
            iterations: int,
            noise_sd: Union[float, Tuple[float, float]],
            relative_noise: bool = False,
            test_size: float = .2,
            score_func: Callable[[np.ndarray, np.ndarray], float] =
            mean_squared_error
    ) -> Tuple[float, float]:
        """
        Fits a regression model to training data generated by the oracle. The
        inputs of the model are spatio-temporal coordinates and the value of
        the solution at the coordinates and its outputs are the value of the
        solution at the next time step. The training data is generated by
        using the oracle to solve sub-IVPs with randomised initial conditions
        and a time domain extent matching the step size of this operator.

        :param ivp: the IVP to train the regression model on
        :param oracle: the operator providing the training data
        :param model: the model to fit to the training data
        :param iterations: the number of data generation iterations
        :param noise_sd: the standard deviation of the Gaussian noise to add to
            the initial conditions of the sub-IVPs. It can be either a scalar,
            in which case the noise is sampled from the same distribution for
            each sub-IVP, or a tuple of two values. The first value of the
            tuple is the standard deviation of the distribution from which the
            noise added to the first sub-IVP is sampled and the second value of
            the tuple is the standard deviation of the distribution from which
            the noise added to the last sub-IVP is sampled. The standard
            deviations of the distribution associated with the sub-IVPs in
            between are calculated using linear interpolation.
        :param relative_noise: whether the noise standard deviation is relative
            to the value of the initial conditions of the sub-IVPs
        :param test_size: the fraction of all data points that should be used
            for testing
        :param score_func: the prediction scoring function to use
        :return: the training and test losses
        """
        if iterations <= 0:
            raise ValueError

        if isinstance(noise_sd, (tuple, list)):
            if len(noise_sd) != 2:
                raise ValueError
            if noise_sd[0] < 0. or noise_sd[1] < 0.:
                raise ValueError
        else:
            if not isinstance(noise_sd, float):
                raise ValueError
            if noise_sd < 0.:
                raise ValueError

            noise_sd = (noise_sd, noise_sd)

        cp = ivp.constrained_problem
        diff_eq = cp.differential_equation

        n_spatial_points = np.prod(cp.mesh.shape(self._vertex_oriented)) \
            if diff_eq.x_dimension else 1

        time_points = discretise_time_domain(ivp.t_interval, self._d_t)
        last_sub_ivp_start_time_point = len(time_points) - 2

        x_batch = self._create_input_batch(cp, time_points[:-1])
        x_batch = np.concatenate(
            (x_batch, np.empty((x_batch.shape[0], diff_eq.y_dimension))),
            axis=-1)
        all_x = np.tile(x_batch, (iterations, 1))

        y_0 = ivp.initial_condition.discrete_y_0(self._vertex_oriented)
        all_y = np.empty((all_x.shape[0], diff_eq.y_dimension))

        for epoch in range(iterations):
            offset = epoch * x_batch.shape[0]
            y_i = y_0

            for i, t_i in enumerate(time_points[:-1]):
                if len(time_points) > 2:
                    interpolated_noise_sd = \
                        (noise_sd[0] * (last_sub_ivp_start_time_point - i) +
                         noise_sd[1] * i) / last_sub_ivp_start_time_point
                else:
                    interpolated_noise_sd = noise_sd[0]
                if relative_noise:
                    interpolated_noise_sd = np.abs(y_i * interpolated_noise_sd)

                y_i += np.random.normal(
                    loc=0.,
                    scale=interpolated_noise_sd,
                    size=y_i.shape).astype(y_i.dtype)

                time_point_offset = offset + i * n_spatial_points
                all_x[time_point_offset:time_point_offset + n_spatial_points,
                      -diff_eq.y_dimension:] = \
                    y_i.reshape((-1, diff_eq.y_dimension))

                sub_ivp = InitialValueProblem(
                    cp,
                    (t_i, t_i + self._d_t),
                    DiscreteInitialCondition(cp, y_i, self._vertex_oriented))
                solution = oracle.solve(sub_ivp)

                y_i = solution.discrete_y(self._vertex_oriented)[-1, ...]
                all_y[time_point_offset:time_point_offset + n_spatial_points,
                      :] = y_i.reshape((-1, diff_eq.y_dimension))

        train_score, test_score = train_model(
            model, all_x, all_y, test_size, score_func)
        self._model = model

        return train_score, test_score

    def _create_input_placeholder(
            self,
            cp: ConstrainedProblem
    ) -> np.ndarray:
        """
        Creates a placeholder array for the ML model inputs. If the constrained
        problem is an ODE, it returns an empty array of shape (1, 1) into which
        t can be substituted to create x. If the constrained problem is a PDE,
        it returns an array of shape (n_mesh_points, 1 + x_dimension) whose
        each row is populated with the spatial coordinates of the corresponding
        mesh point in addition to an empty leading column for t.
        :param cp: the constrained problem to base the inputs on
        :return: the placeholder array for the ML inputs
        """
        diff_eq = cp.differential_equation

        if diff_eq.x_dimension:
            x = cp.mesh.all_x(self._vertex_oriented)
            x = np.hstack((np.empty((x.shape[0], 1)), x))
        else:
            x = np.empty((1, 1))

        return x

    def _create_input_batch(
            self,
            cp: ConstrainedProblem,
            time_points: np.ndarray
    ) -> np.ndarray:
        """
        Creates a 2D array of inputs with a shape of
        (n_mesh_points * n_time_points, x_dimension + 1).
        :param cp: the constrained problem to base the inputs on
        :param time_points: the discretised time domain of the IVP to create
            inputs for
        :return: a batch of all inputs
        """
        input_placeholder = self._create_input_placeholder(cp)
        n_mesh_points = input_placeholder.shape[0]

        x = np.tile(input_placeholder, (len(time_points), 1))
        t = np.repeat(time_points, n_mesh_points)
        x[:, 0] = t

        return x


def train_model(
        model: RegressionModel,
        x: np.ndarray,
        y: np.ndarray,
        test_size: float,
        score_func: Callable[[np.ndarray, np.ndarray], float]
) -> Tuple[float, float]:
    """
    Fits the regression model to the training share of the provided data
    points using random splitting and it returns the loss of the model
    evaluated on both the training and test data sets.

    :param model: the regression model to train
    :param x: the inputs
    :param y: the target outputs
    :param test_size: the fraction of all data points that should be used
        for testing
    :param score_func: the prediction scoring function to use
    :return: the training and test losses
    """
    if not 0. <= test_size < 1.:
        raise ValueError
    train_size = 1. - test_size

    x_train, x_test, y_train, y_test = train_test_split(
        x,
        y,
        train_size=train_size,
        test_size=test_size)

    model.fit(x_train, y_train)

    y_train_hat = model.predict(x_train)
    y_test_hat = model.predict(x_test)
    train_score = score_func(y_train, y_train_hat)
    test_score = score_func(y_test, y_test_hat)
    return train_score, test_score
